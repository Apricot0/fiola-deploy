import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
import numpy as np
import h5py
import scipy.io as sio
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
# from sklearn.model_selection import train_test_split
from tensorflow.keras.models import load_model
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score

class NeuralSpikeLSTM:
    def __init__(self, neurons, window_size):
        self.neurons = neurons
        self.window_size = window_size
        self.model = self._build_model()

    def _build_model(self):

        model = Sequential()
        
        model.add(LSTM(64, return_sequences=True, input_shape=(self.window_size, self.neurons)))
        model.add(Dropout(0.2))
        model.add(LSTM(32, return_sequences=False))
        model.add(Dropout(0.2))

        model.add(Dense(32, activation='relu'))
        model.add(Dense(1, activation='sigmoid'))  # Binary classification: light (1) or dark (0)

        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
        return model

    def train(self, X_train, y_train, epochs=20, batch_size=32, validation_split=0.2):
        self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)

    def predict(self, X_test):
        probabilities = self.model.predict(X_test)
        binary_predictions = (probabilities >= 0.5).astype(int) 
        return binary_predictions

    def save_model(self, file_path="/persistent_storage/pre_trained_model.h5"):
        self.model.save(file_path)
    @classmethod
    def load_trained_model(cls, file_path="/persistent_storage/fine_tuned_model.h5"):
        return load_model(file_path)

    def fine_tune(self, new_neurons, weights_path):
        new_model = NeuralSpikeLSTM(new_neurons, self.window_size)
        new_model.model.load_weights(weights_path, by_name=True, skip_mismatch=True)
        self.model = new_model.model  
    
    def evaluate(self, X_val, y_val):
        loss, accuracy = self.model.evaluate(X_val, y_val, verbose=0)
        print(f"Validation Loss: {loss:.4f}, Accuracy: {accuracy:.4f}")
        return loss, accuracy

def get_max_neurons(file_list):
    """
    Loop through a list of .mat files and return the maximum number
    of spike channels found. This is used to pad spike data to a fixed width.
    """
    max_neurons = 0
    for file in file_list:
        data = sio.loadmat(file)
        spks = data['spks']  # shape: (n_neurons, n_frames)
        n_neurons = spks.shape[0]
        if n_neurons > max_neurons:
            max_neurons = n_neurons
    return max_neurons

def load_mat_file(filepath, max_neurons = None):
    """
    Load a MAT file generated by CaImAn and return a merged feature matrix.
    
    Assumes the MAT file contains:
      - t         : (n_frames, 1)   Time stamps
      - arena_x   : (n_frames, 1)
      - arena_y   : (n_frames, 1)
      - spks      : (n_neurons, n_frames)  Spike traces
    
    This function transposes spks so that each row corresponds to one frame,
    resulting in an array of shape: (n_frames, max_neurons)
    """
    data = sio.loadmat(filepath)
    spks = data['spks']        # shape: (n_neurons, n_frames)
    t=data['t'].flatten() 
    # Transpose spks so that rows correspond to frames
    spks = spks.T             # new shape: (n_frames, n_neurons)
    n_frames, n_neurons = spks.shape
    
    # Pad spike data with zeros to reach max_neurons columns if needed.
    if max_neurons and n_neurons < max_neurons:
        padding = np.zeros((n_frames, max_neurons - n_neurons))
        spks = np.concatenate([spks, padding], axis=1)
    
    return spks, t

def spikes_reader(file_path):
    with h5py.File(file_path, "r") as f:
        estimates_group = f['estimates']
        
        # Check if 'S' exists in the estimates group
        if 'S' in estimates_group:
            spike_activity = estimates_group['S'][:]
            print("Shape of spike activity (S):", spike_activity.shape)
        else:
            raise ValueError("'S' dataset not found in the file.")
    return spike_activity

def create_sliding_windows(data, labels, window_size=50, step=1):
    """
    neural spike activity to LSTM sequences.
    Args:
        data: (timestamps, neurons)
        labels:  (light=1, dark=0)
        window_size: timestamps per sequence
    Returns:
        X: (samples, window_size, neurons)
        y: (light or dark)
    """
    X, y = [], []
    
    for i in range(0, len(data) - window_size, step):
        X.append(data[i : i + window_size])  # Extract window
        y.append(labels[i + window_size])  # Use the label at the end of window

    return np.array(X), np.array(y)


def load_and_finetune_model(spike_activity, weights_path, window_size, step_size, epochs, batch_size, labels, neurons):
    """
    Loads the spikes from calman init file and fine-tunes an LSTM model.
    
    Args:
        file_path: Path to caiman spike activity hdf5.
        weights_path: Path to weights.
        window_size
        step_size
        epochs
        batch_size
        labels:  (light=1, dark=0).
    Returns:
        model: Fine-tuned LSTM model.
    """

    X, y = create_sliding_windows(spike_activity, labels, window_size, step_size)
    
    model = NeuralSpikeLSTM(neurons, window_size)
    model.fine_tune(neurons, weights_path) 
    model.train(X, y, epochs=epochs, batch_size=batch_size)
    fine_tuned_weights_path = "/persistent_storage/fine_tuned_model.h5"
    model.save_model(fine_tuned_weights_path)
    print(f"Fine-tuned model saved to {fine_tuned_weights_path}")
    
    return model
def train_model_from_scratch(spike_activity, window_size, step_size, epochs, batch_size, labels, neurons):
    """
    Trains a new LSTM model from scratch using spike activity and labels.

    Args:
        spike_activity: Array of spike activity from CaImAn or similar.
        window_size: Size of the sliding window.
        step_size: Step size between sliding windows.
        epochs: Number of training epochs.
        batch_size: Training batch size.
        labels: Ground truth labels (light=1, dark=0).
    
    Returns:
        model: Trained LSTM model.
    """
    X, y = create_sliding_windows(spike_activity, labels, window_size, step_size)
    model = NeuralSpikeLSTM(neurons, window_size)

    model.train(X, y, epochs=epochs, batch_size=batch_size)

    trained_weights_path = "/persistent_storage/trained_model_from_scratch.h5"
    model.save_model(trained_weights_path)
    print(f"Trained model saved to {trained_weights_path}")

    return model

def train_model_from_scratch_with_kfold(spike_activity, window_size, step_size, epochs, batch_size, labels, neurons, k_folds=5):
    """
    Trains an LSTM model using K-Fold cross-validation and returns the best-performing model.

    Args:
        spike_activity: Array of spike activity (frames x neurons).
        window_size: Size of the sliding window.
        step_size: Step between windows.
        epochs: Number of training epochs.
        batch_size: Training batch size.
        labels: Binary labels for supervised training (light=1, dark=0).
        k_folds: Number of folds for cross-validation.

    Returns:
        best_model: The model with the highest validation accuracy.
    """
    X, y = create_sliding_windows(spike_activity, labels, window_size, step_size)
    X, y = np.array(X), np.array(y)

    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)
    best_model = None
    best_accuracy = -1.0

    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
        print(f"\n--- Fold {fold + 1}/{k_folds} ---")

        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]

        model = NeuralSpikeLSTM(neurons, window_size)
        model.train(X_train, y_train, epochs=epochs, batch_size=batch_size)

        val_accuracy = model.evaluate(X_val, y_val)
        print(f"Fold {fold + 1} Validation Accuracy: {val_accuracy:.4f}")

        model_path = "/persistent_storage/trained_model_from_scratch.h5"
        model.save_model(model_path)
        print(f"Model for fold {fold + 1} saved to {model_path}")

        # Update best model
        if val_accuracy > best_accuracy:
            best_accuracy = val_accuracy
            best_model = model
            model_path = f"trained_model_fold{fold + 1}.h5"
            model.save_model(model_path)
            print(f"New best model found at fold {fold + 1} with accuracy {best_accuracy:.4f}")

    return best_model
def load_and_merge_spike_data(light_file, dark_file, max_neurons):
    """
    merge 1 animal's spks and add give them labels, sorted them according to timestamp. 
    """
    spikes_light, t_light = load_mat_file(light_file, max_neurons)
    spikes_dark, t_dark = load_mat_file(dark_file, max_neurons)

    y_light = np.ones_like(t_light) 
    y_dark = np.zeros_like(t_dark) 

    t_combined = np.concatenate((t_light, t_dark))
    spikes_combined = np.vstack((spikes_light, spikes_dark))
    y_combined = np.concatenate((y_light, y_dark))

    sorted_indices = np.argsort(t_combined)
    X_sorted = spikes_combined[sorted_indices]
    y_sorted = y_combined[sorted_indices]


    return X_sorted, y_sorted
def main():

    light_files = ['light_1.mat', 'light_2.mat']
    dark_files  = ['dark_1.mat', 'dark_2.mat']
    all_files = light_files + dark_files
    
    max_neurons = get_max_neurons(all_files)
    print("Maximum number of neurons found across sessions:", max_neurons)
    
    window_size = 50  

    X_list = np.empty((0, window_size, max_neurons))
    y_list = np.empty((0,))

    for i in range(1, 3):
        features, labels = load_and_merge_spike_data(f'light_{i}.mat', f'dark_{i}.mat', max_neurons)
        sequences, sliced_label = create_sliding_windows(features, labels, window_size, step=1)
        X_list = np.vstack([X_list, sequences])
        y_list = np.concatenate([y_list, sliced_label])
    
    print("Feature:", X_list.shape)
    print("Label:", y_list.shape)

    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    
    fold = 1
    accuracies = []
    for train_idx, val_idx in kf.split(X_list):
        print(f"Training fold {fold}")

        X_train, X_val = X_list[train_idx], X_list[val_idx]
        y_train, y_val = y_list[train_idx], y_list[val_idx]

        model = NeuralSpikeLSTM(neurons=max_neurons, window_size=window_size)
        model.train(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)
        y_pred = model.predict(X_val)
        y_pred = (y_pred >= 0.5).astype(int)  
        accuracy = accuracy_score(y_val, y_pred)
        accuracies.append(accuracy)
        print(f"Fold {fold} Accuracy: {accuracy:.4f}")
        fold += 1

    avg_accuracy = np.mean(accuracies)
    print(f"Average Accuracy across all folds: {avg_accuracy:.4f}")

    model.save_model()
    print("Model saved")

if __name__ == "__main__":
    main()